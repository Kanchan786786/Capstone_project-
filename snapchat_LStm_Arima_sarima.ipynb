{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPd1C7c04gAQ+KO7elq1rp3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kanchan786786/Capstone_project-/blob/main/snapchat_LStm_Arima_sarima.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X8Lxo6epYwhr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-tuner --upgrade\n"
      ],
      "metadata": {
        "id": "d06A0H3lgyqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vJUkjESoZyCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "014fad9c"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "from kerastuner.tuners import BayesianOptimization"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install yfinance\n"
      ],
      "metadata": {
        "id": "JL-269bVZ6T4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ta\n"
      ],
      "metadata": {
        "id": "ksFOB3Wrt12S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Install required libraries\n",
        "# # !pip install yfinance pandas ta\n",
        "\n",
        "# import yfinance as yf\n",
        "# import pandas as pd\n",
        "# from ta.trend import sma_indicator, ema_indicator, MACD\n",
        "# from ta.momentum import RSIIndicator\n",
        "\n",
        "# # Define Snapchat ticker symbol\n",
        "# symbol = 'SNAP'\n",
        "\n",
        "# # Set time period for data retrieval\n",
        "# start_date = '2019-03-29'\n",
        "# end_date = '2024-12-31'\n",
        "\n",
        "# # Download data with auto_adjust=True by default\n",
        "# snap_data = yf.download(symbol, start=start_date, end=end_date)\n",
        "\n",
        "# # Calculate VWAP\n",
        "# snap_data['VWAP'] = (snap_data['Volume'] * (snap_data['High'] + snap_data['Low'] + snap_data['Close']) / 3).cumsum() / snap_data['Volume'].cumsum()\n",
        "\n",
        "# # Extract close series\n",
        "# close_series = snap_data['Close'].squeeze()\n",
        "\n",
        "# # Add technical indicators\n",
        "# snap_data['SMA_50'] = sma_indicator(close_series, window=50)\n",
        "# snap_data['EMA_50'] = ema_indicator(close_series, window=50)\n",
        "# snap_data['RSI'] = RSIIndicator(close_series).rsi()\n",
        "# macd = MACD(close_series)\n",
        "# snap_data['MACD'] = macd.macd()\n",
        "# snap_data['MACD_Signal'] = macd.macd_signal()\n",
        "# snap_data['MACD_Hist'] = macd.macd_diff()\n",
        "\n",
        "# # Use Close as proxy for Adjusted Close since auto_adjust=True by default\n",
        "# snap_data['Adjusted_Close'] = snap_data['Close']\n",
        "\n",
        "# # Market Cap approximation (close price * volume)\n",
        "# snap_data['Market_Cap'] = snap_data['Close'] * snap_data['Volume']\n",
        "\n",
        "# # Create Tomorrow's Close feature\n",
        "# snap_data['Tomorrow'] = snap_data['Close'].shift(-1)\n",
        "\n",
        "# # Select required features\n",
        "# desired_features = ['Close', 'Tomorrow', 'Low', 'Open', 'Volume', 'VWAP', 'Adjusted_Close', 'Market_Cap', 'SMA_50', 'EMA_50', 'RSI', 'MACD', 'MACD_Signal', 'MACD_Hist', 'High']\n",
        "# snap_data = snap_data[desired_features].dropna()\n",
        "\n",
        "# # Reset index for convenience\n",
        "# snap_data.reset_index(inplace=True)\n",
        "\n",
        "# # Display the first few rows of the dataset\n",
        "# print(snap_data.head())\n",
        "\n",
        "# # Save the dataset to a CSV file\n",
        "# snap_data.to_csv('snapchat_stock_data_features.csv', index=False)"
      ],
      "metadata": {
        "id": "Lc2b04t7cYmj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = 'https://raw.githubusercontent.com/Kanchan786786/Capstone_project-/main/snapchat_stock_data_features.csv'\n",
        "df = pd.read_csv(url)"
      ],
      "metadata": {
        "id": "4u_aXaO6ckCP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "id": "mI_EASZldMKF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: drop the first row\n",
        "\n",
        "df = df.iloc[1:]\n",
        "df.tail()\n"
      ],
      "metadata": {
        "id": "vpx_efIXdNvY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: see the missing values\n",
        "\n",
        "print(df.isnull().sum())\n"
      ],
      "metadata": {
        "id": "pPLEBdSKdlNB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Select features and target variable\n",
        "features = ['High', 'Low', 'Open', 'Volume', 'VWAP', 'Adjusted_Close', 'Market_Cap', 'SMA_50', 'EMA_50', 'RSI', 'MACD']\n",
        "target = 'Close'\n",
        "\n",
        "# Normalize data using MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "df_scaled = pd.DataFrame(scaler.fit_transform(df[features + [target]]), columns=features + [target], index=df.index)\n",
        "\n",
        "# Prepare sequences for LSTM\n",
        "def create_sequences(data, feature_cols, target_col, time_steps=60):\n",
        "    np.random.seed(42)\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - time_steps):\n",
        "        X.append(data[feature_cols].iloc[i:i+time_steps].values)\n",
        "        y.append(data[target_col].iloc[i+time_steps])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "time_steps = 60  # Use past 60 days for prediction\n",
        "X, y = create_sequences(df_scaled, features, target, time_steps)\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Step 1: First split into temp (85%) and test (15%)\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=0.15, shuffle=False, random_state=42)\n",
        "\n",
        "# Step 2: Split temp into train (70%) and validation (15%)\n",
        "# Note: 15% / 85% ≈ 0.1765 — so we split 17.65% from temp to get 15% of total as val\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.1765, shuffle=False, random_state=42)\n",
        "\n",
        "\n",
        "# # Build the LSTM model\n",
        "# model = Sequential([\n",
        "#     LSTM(50, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
        "#     Dropout(0.2),\n",
        "#     LSTM(50, return_sequences=False),\n",
        "#     Dropout(0.2),\n",
        "#     Dense(25, activation='relu'),\n",
        "#     Dense(1)\n",
        "# ])\n",
        "\n",
        "# # Compile the model\n",
        "# model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# # Train the model\n",
        "# history = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# # Make predictions\n",
        "# predictions = model.predict(X_test)\n",
        "\n",
        "# # Inverse transform the predictions to the original scale\n",
        "# predictions_inv = scaler.inverse_transform(np.concatenate((X_test[:, -1, :], predictions.reshape(-1, 1)), axis=1))[:, -1]\n",
        "# y_test_inv = scaler.inverse_transform(np.concatenate((X_test[:, -1, :], y_test.reshape(-1, 1)), axis=1))[:, -1]\n",
        "\n"
      ],
      "metadata": {
        "id": "DDD71xOxdwgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape, X_val.shape, X_test.shape"
      ],
      "metadata": {
        "id": "3LXKDqMVeUg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7Qd_GxJ4w-I"
      },
      "outputs": [],
      "source": [
        "# # prompt: plot a graph little clear with the advance and clear details , x axis dates should be claer like it shiuld not over lap\n",
        "\n",
        "# import pandas as pd\n",
        "# import matplotlib.pyplot as plt\n",
        "# import matplotlib.dates as mdates\n",
        "\n",
        "# # ... (Your existing code)\n",
        "\n",
        "# # Plot actual vs predicted values\n",
        "# plt.figure(figsize=(12, 6))\n",
        "\n",
        "# # Convert 'Date' column to datetime objects\n",
        "# df['Date'] = pd.to_datetime(df['Date'])\n",
        "\n",
        "# plt.plot(df['Date'].iloc[-len(y_test):], y_test_inv, label=\"Actual Prices\")\n",
        "# plt.plot(df['Date'].iloc[-len(y_test):], predictions_inv, label=\"Predicted Prices\", linestyle='dashed')\n",
        "# plt.xlabel(\"Date\")\n",
        "# plt.ylabel(\"Price\")\n",
        "# plt.legend()\n",
        "# plt.title(\"Actual vs Predicted Prices\")\n",
        "\n",
        "# # Format the x-axis for better date display\n",
        "# plt.gca().xaxis.set_major_locator(mdates.AutoDateLocator())\n",
        "# plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d')) # Customize the date format as needed\n",
        "# plt.gcf().autofmt_xdate() # Rotate date labels for better readability\n",
        "\n",
        "# plt.tight_layout() # Adjust layout to prevent labels from overlapping\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-tuner\n"
      ],
      "metadata": {
        "id": "Ys3k1ElXfgN9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from keras_tuner import RandomSearch\n",
        "\n",
        "# Define the model function for tuning\n",
        "def build_model(hp):\n",
        "    model = Sequential([\n",
        "        LSTM(hp.Int('units1', min_value=50, max_value=200, step=50), return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])),\n",
        "        Dropout(hp.Choice('dropout_rate', values=[0.2, 0.3, 0.4])),\n",
        "        LSTM(hp.Int('units2', min_value=50, max_value=200, step=50), return_sequences=False),\n",
        "        Dropout(hp.Choice('dropout_rate', values=[0.2, 0.3, 0.4])),\n",
        "        Dense(25, activation=hp.Choice('activation', ['relu', 'tanh'])),\n",
        "        Dense(1)\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "    return model\n",
        "\n",
        "# Initialize the tuner\n",
        "tuner = RandomSearch(\n",
        "    build_model,\n",
        "    objective='val_loss',\n",
        "    max_trials= 5,  # Number of different models to try\n",
        "    executions_per_trial=1,  # Number of times to train each model\n",
        "    directory='lstm_tuner',\n",
        "    project_name='stock_prediction'\n",
        ")\n",
        "\n",
        "# Run the hyperparameter search\n",
        "tuner.search(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_val, y_val))\n",
        "\n",
        "# Get the best hyperparameters\n",
        "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "# Build the best model\n",
        "best_model = tuner.hypermodel.build(best_hps)\n",
        "\n",
        "# Train the best model\n",
        "history = best_model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_val, y_val))\n",
        "\n",
        "# Evaluate\n",
        "best_model.evaluate(X_test, y_test)\n"
      ],
      "metadata": {
        "id": "fEdUYTq_eyd6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "ZTviW0cpkH2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, median_absolute_error, explained_variance_score\n",
        "\n"
      ],
      "metadata": {
        "id": "VwkuIVPyfbxA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# # Evaluate the model on the training data\n",
        "# train_predictions =best_model.predict(X_train)\n",
        "# train_predictions_inv = scaler.inverse_transform(np.concatenate((X_train[:, -1, :], train_predictions.reshape(-1, 1)), axis=1))[:, -1]\n",
        "# y_train_inv = scaler.inverse_transform(np.concatenate((X_train[:, -1, :], y_train.reshape(-1, 1)), axis=1))[:, -1]\n",
        "# For test data\n",
        "test_predictions =best_model.predict(X_test)\n",
        "y_test_inv = scaler.inverse_transform(np.concatenate((X_test[:, -1, :], y_test.reshape(-1, 1)), axis=1))[:, -1]\n",
        "predictions_inv = scaler.inverse_transform(np.concatenate((X_test[:, -1, :], test_predictions.reshape(-1, 1)), axis=1))[:, -1]\n",
        "\n",
        "# train_mse = mean_squared_error(y_train_inv, train_predictions_inv)\n",
        "# train_rmse = np.sqrt(train_mse)\n",
        "# train_r2 = r2_score(y_train_inv, train_predictions_inv)\n",
        "\n",
        "# print(f\"Training Accuracy (R-squared): {train_r2}\")\n",
        "# print(f\"Training MSE: {train_mse}\")\n",
        "# print(f\"Training RMSE: {train_rmse}\")\n"
      ],
      "metadata": {
        "id": "B6xYBnzdp14w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.dates as mdates\n"
      ],
      "metadata": {
        "id": "BwYPkDFQWw41"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Assuming your date column is named 'Date'\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n"
      ],
      "metadata": {
        "id": "77MMEKaFXVZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fd2I1PMHscLa"
      },
      "outputs": [],
      "source": [
        "# prompt: generate the code the error metric just like model2 for my best model as well\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Make predictions with the best model\n",
        "best_predictions = best_model.predict(X_test)\n",
        "\n",
        "# Inverse transform the predictions to the original scale\n",
        "best_predictions_inv = scaler.inverse_transform(np.concatenate((X_test[:, -1, :], best_predictions.reshape(-1, 1)), axis=1))[:, -1]\n",
        "\n",
        "# Calculate evaluation metrics for the best model\n",
        "best_mse = mean_squared_error(y_test_inv, best_predictions_inv)\n",
        "best_rmse = np.sqrt(best_mse)\n",
        "best_r2 = r2_score(y_test_inv, best_predictions_inv)\n",
        "\n",
        "# Create a table of evaluation metrics for the best model\n",
        "best_evaluation_metrics = pd.DataFrame({\n",
        "    'Metric': ['Mean Squared Error (MSE)', 'Root Mean Squared Error (RMSE)', 'R-squared (R2)'],\n",
        "    'Value': [best_mse, best_rmse, best_r2]\n",
        "})\n",
        "\n",
        "print(\"\\nEvaluation Metrics for Best Model:\\n\")\n",
        "print(best_evaluation_metrics)\n",
        "\n",
        "# # Evaluate the best model on the training data\n",
        "# best_train_predictions = best_model.predict(X_train)\n",
        "# best_train_predictions_inv = scaler.inverse_transform(np.concatenate((X_train[:, -1, :], best_train_predictions.reshape(-1, 1)), axis=1))[:, -1]\n",
        "\n",
        "# best_train_mse = mean_squared_error(y_train_inv, best_train_predictions_inv)\n",
        "# best_train_rmse = np.sqrt(best_train_mse)\n",
        "# best_train_r2 = r2_score(y_train_inv, best_train_predictions_inv)\n",
        "\n",
        "# print(f\"\\nBest Model - Training Accuracy (R-squared): {best_train_r2}\")\n",
        "# print(f\"Best Model - Training MSE: {best_train_mse}\")\n",
        "# print(f\"Best Model - Training RMSE: {best_train_rmse}\")\n",
        "\n",
        "print(f\"\\nBest Model - Testing Accuracy (R-squared): {best_r2}\")\n",
        "print(f\"Best Model - Testing MSE: {best_mse}\")\n",
        "print(f\"Best Model - Testing RMSE: {best_rmse}\")\n",
        "\n",
        "#Plot for the best model\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(df['Date'].iloc[-len(y_test):], y_test_inv, label=\"Actual Prices\")\n",
        "plt.plot(df['Date'].iloc[-len(y_test):], best_predictions_inv, label=\"Predicted Prices (Best Model)\", linestyle='dashed')\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Price\")\n",
        "plt.legend()\n",
        "plt.title(\"Actual vs Predicted Prices (Best Model)\")\n",
        "plt.gca().xaxis.set_major_locator(mdates.AutoDateLocator())\n",
        "plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m-%d'))\n",
        "plt.gcf().autofmt_xdate()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model builder for KerasTuner\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop"
      ],
      "metadata": {
        "id": "loz8Jl6obYw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(hp):\n",
        "    model = Sequential()\n",
        "    for i in range(hp.Int('num_layers', 1, 3)):\n",
        "        return_sequences = i < hp.get('num_layers') - 1\n",
        "        model.add(LSTM(units=hp.Int(f'units_{i}', min_value=32, max_value=128, step=32),\n",
        "                       return_sequences=return_sequences,\n",
        "                       input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "        model.add(Dropout(rate=hp.Float(f'dropout_{i}', min_value=0.1, max_value=0.5, step=0.1)))\n",
        "    model.add(Dense(1))\n",
        "    optimizer = hp.Choice('optimizer', ['adam', 'rmsprop'])\n",
        "    lr = hp.Float('learning_rate', 1e-4, 1e-2, sampling='log')\n",
        "    model.compile(loss='mse', optimizer=Adam(learning_rate=lr) if optimizer=='adam' else RMSprop(learning_rate=lr))\n",
        "    return model"
      ],
      "metadata": {
        "id": "2IRhFv05hEiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tuner setup\n",
        "tuner = BayesianOptimization(\n",
        "    build_model,\n",
        "    objective='val_loss',\n",
        "    max_trials= 8,\n",
        "    executions_per_trial=1,\n",
        "    directory='lstm_tuning',\n",
        "    project_name='stock_lstm')"
      ],
      "metadata": {
        "id": "9xnaEPZhhJh-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Callbacks\n",
        "early_stop = EarlyStopping(patience=5, restore_best_weights=True)\n",
        "reduce_lr = ReduceLROnPlateau(patience=3, factor=0.5)"
      ],
      "metadata": {
        "id": "bxSpwNxmhN9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tuner.search(X_train, y_train, epochs=30, validation_split=0.2, callbacks=[early_stop, reduce_lr])"
      ],
      "metadata": {
        "id": "DpR9_9mThRZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get best model\n",
        "best_model_2 = tuner.get_best_models(num_models=1)[0]"
      ],
      "metadata": {
        "id": "0ETixR_8hW66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = build_model(tuner.oracle.get_best_trials(num_trials=1)[0].hyperparameters)\n",
        "model.fit(X_train, y_train, epochs=30, batch_size=32, validation_split=0.2,\n",
        "          callbacks=[early_stop, reduce_lr])"
      ],
      "metadata": {
        "id": "kjjwgxIVjMYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds_2 = model.predict(X_val)\n",
        "mse_2 = mean_squared_error(y_val, preds_2)\n",
        "print(\"Test MSE:\", mse_2)"
      ],
      "metadata": {
        "id": "2rVqEscdvhzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate\n",
        "preds = model.predict(X_test)\n",
        "mse = mean_squared_error(y_test, preds)\n",
        "print(\"Test MSE:\", mse)"
      ],
      "metadata": {
        "id": "1-zolojtjY5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.plot(y_test, label='True')\n",
        "plt.plot(preds, label='Predicted')\n",
        "plt.legend()\n",
        "plt.title('LSTM Stock Prediction')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "oNV9vRVIjwSG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# # Training R-squared\n",
        "# train_preds = model.predict(X_train)\n",
        "# train_r2 = r2_score(y_train, train_preds)\n",
        "# print(f\"Training R-squared: {train_r2}\")\n",
        "\n",
        "\n",
        "test_r2 = r2_score(y_test, preds)\n",
        "print(f\"Testing R-squared: {test_r2}\")"
      ],
      "metadata": {
        "id": "G2YsClhdj0z5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# # Training R-squared\n",
        "# train_preds = model.predict(X_train)\n",
        "# train_r2 = r2_score(y_train, train_preds)\n",
        "# print(f\"Training R-squared: {train_r2}\")\n",
        "\n",
        "\n",
        "val_r2 = r2_score(y_val, preds_2)\n",
        "print(f\"Validation R-squared: {val_r2}\")"
      ],
      "metadata": {
        "id": "HN5NkqoUvwv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sarima And Arima"
      ],
      "metadata": {
        "id": "UToh7BqIXDa2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/Kanchan786786/Capstone_project-/main/snapchat_stock_data_features.csv'\n",
        "df_arima = pd.read_csv(url)\n",
        "\n",
        "# Preview the data\n",
        "df_arima.head()\n"
      ],
      "metadata": {
        "id": "f0LKssQwMVUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_info = df_arima.info()\n",
        "df_head = df_arima.head()\n",
        "df_describe = df_arima.describe()\n",
        "\n",
        "df_head, df_info, df_describe"
      ],
      "metadata": {
        "id": "QFFhu5-bMVIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "\n",
        "# Step 1: Convert Date to datetime and set as index\n",
        "df_arima['Date'] = pd.to_datetime(df_arima['Date'])\n",
        "df_arima.set_index('Date', inplace=True)\n",
        "\n",
        "# Step 2: Extract the target time series (we'll use 'Close' for now)\n",
        "ts = df_arima['Close']\n",
        "\n",
        "# Step 3: Plot the time series\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(ts, label='Snapchat Close Price')\n",
        "plt.title('Snapchat Stock - Close Price Over Time')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Price')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Step 4: Perform Augmented Dickey-Fuller test\n",
        "adf_result = adfuller(ts)\n",
        "\n",
        "adf_summary = {\n",
        "    'ADF Statistic': adf_result[0],\n",
        "    'p-value': adf_result[1],\n",
        "    'Used Lag': adf_result[2],\n",
        "    'Number of Observations': adf_result[3],\n",
        "    'Critical Values': adf_result[4],\n",
        "    'Stationary?': 'Yes' if adf_result[1] < 0.05 else 'No'\n",
        "}\n",
        "\n",
        "adf_summary\n"
      ],
      "metadata": {
        "id": "xsoe-NRvMU4O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: First-order differencing to make the series stationary\n",
        "ts_diff = ts.diff().dropna()\n",
        "\n",
        "# Plot differenced series\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(ts_diff, label='1st Order Differenced Close Price', color='orange')\n",
        "plt.title('Differenced Snapchat Close Price (1st Order)')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Differenced Price')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ADF test on differenced series\n",
        "adf_diff_result = adfuller(ts_diff)\n",
        "\n",
        "adf_diff_summary = {\n",
        "    'ADF Statistic': adf_diff_result[0],\n",
        "    'p-value': adf_diff_result[1],\n",
        "    'Used Lag': adf_diff_result[2],\n",
        "    'Number of Observations': adf_diff_result[3],\n",
        "    'Critical Values': adf_diff_result[4],\n",
        "    'Stationary?': 'Yes' if adf_diff_result[1] < 0.05 else 'No'\n",
        "}\n",
        "\n",
        "adf_diff_summary\n"
      ],
      "metadata": {
        "id": "l4t9g6crIFL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split data: 85% train, 15% test (chronologically)\n",
        "split_index = int(len(ts) * 0.85)\n",
        "ts_train = ts.iloc[:split_index]\n",
        "ts_test = ts.iloc[split_index:]\n",
        "\n",
        "# Confirm the split sizes\n",
        "train_len = len(ts_train)\n",
        "test_len = len(ts_test)\n",
        "\n",
        "train_len, test_len, ts_train.tail(), ts_test.head()\n"
      ],
      "metadata": {
        "id": "ISaWs2-xIPsg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "\n",
        "# Use differenced training data for ACF/PACF\n",
        "ts_train_diff = ts_train.diff().dropna()\n",
        "\n",
        "# Plot ACF and PACF\n",
        "fig, ax = plt.subplots(2, 1, figsize=(12, 8))\n",
        "\n",
        "plot_acf(ts_train_diff, lags=40, ax=ax[0])\n",
        "ax[0].set_title('ACF - Differenced Training Series')\n",
        "\n",
        "plot_pacf(ts_train_diff, lags=40, ax=ax[1])\n",
        "ax[1].set_title('PACF - Differenced Training Series')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "q8qydoAhIuOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "\n",
        "# Fit ARIMA(1,1,1) on training data\n",
        "arima_model = ARIMA(ts_train, order=(1, 1, 1))\n",
        "arima_result = arima_model.fit()\n",
        "\n",
        "# Forecast for the same length as test set\n",
        "arima_forecast = arima_result.forecast(steps=len(ts_test))\n",
        "\n",
        "# Plot actual vs forecasted\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(ts_train, label='Training Data')\n",
        "plt.plot(ts_test, label='Actual Test Data')\n",
        "plt.plot(ts_test.index, arima_forecast, label='ARIMA Forecast', color='red')\n",
        "plt.title('ARIMA(1,1,1) Forecast vs Actual (Test Set)')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Close Price')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Calculate performance metrics\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "mse_arima = mean_squared_error(ts_test, arima_forecast)\n",
        "mae_arima = mean_absolute_error(ts_test, arima_forecast)\n",
        "\n",
        "mse_arima, mae_arima\n"
      ],
      "metadata": {
        "id": "LF9CCwVzIydm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Calculate R² score for ARIMA predictions\n",
        "r2_arima = r2_score(ts_test, arima_forecast)\n",
        "\n",
        "r2_arima\n"
      ],
      "metadata": {
        "id": "rmC_lnj1J110"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pmdarima\n"
      ],
      "metadata": {
        "id": "f7C95uQqKxSp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "\n",
        "# Apply seasonal decomposition on training set (assuming daily frequency)\n",
        "decomposition = seasonal_decompose(ts_train, model='additive', period=5)  # 5 = weekly pattern\n",
        "\n",
        "# Plot the decomposition\n",
        "decomposition.plot()\n",
        "plt.suptitle(\"Seasonal Decomposition of Snapchat Close Price (Period = 5)\", fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "FphGnxTWLFHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "\n",
        "# Fit SARIMA(1,1,1)(1,0,1,5)\n",
        "sarima_model = SARIMAX(ts_train, order=(1, 1, 1), seasonal_order=(1, 0, 1, 5))\n",
        "sarima_result = sarima_model.fit(disp=False)\n",
        "\n",
        "# Forecast for the length of the test set\n",
        "sarima_forecast = sarima_result.forecast(steps=len(ts_test))\n",
        "\n",
        "# Plot actual vs forecasted\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(ts_train, label='Training Data')\n",
        "plt.plot(ts_test, label='Actual Test Data')\n",
        "plt.plot(ts_test.index, sarima_forecast, label='SARIMA Forecast', color='green')\n",
        "plt.title('SARIMA(1,1,1)(1,0,1,5) Forecast vs Actual (Test Set)')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Close Price')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Evaluate performance\n",
        "mse_sarima = mean_squared_error(ts_test, sarima_forecast)\n",
        "mae_sarima = mean_absolute_error(ts_test, sarima_forecast)\n",
        "\n",
        "mse_sarima, mae_sarima\n"
      ],
      "metadata": {
        "id": "Teg2JooBLE80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Calculate R² score for ARIMA predictions\n",
        "r2_sarima = r2_score(ts_test, sarima_forecast)\n",
        "\n",
        "r2_sarima\n"
      ],
      "metadata": {
        "id": "H1j6s0aKLEzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Log and Scale to transform the dataset and try again the Arima and Sarima to see the trend"
      ],
      "metadata": {
        "id": "fQ-tLmQtHCrB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "BlAhVhOIOAvh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "GbwJYmw5N_XK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the re-uploaded dataset\n",
        "file_path = \"https://raw.githubusercontent.com/Kanchan786786/Capstone_project-/main/snapchat_stock_data_features.csv\"\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Convert Date to datetime and set as index\n",
        "df['Date'] = pd.to_datetime(df['Date'])\n",
        "df.set_index('Date', inplace=True)\n",
        "\n",
        "# Extract target time series\n",
        "ts = df['Close']\n",
        "\n",
        "# Step 1: Log transformation\n",
        "ts_log = np.log(ts)\n",
        "\n",
        "# Step 2: MinMax scaling\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "ts_log_scaled = pd.Series(\n",
        "    scaler.fit_transform(ts_log.values.reshape(-1, 1)).flatten(),\n",
        "    index=ts.index\n",
        ")\n",
        "\n",
        "# Step 3: Split into training and testing (85/15)\n",
        "split_index = int(len(ts_log_scaled) * 0.85)\n",
        "ts_train_scaled = ts_log_scaled.iloc[:split_index]\n",
        "ts_test_scaled = ts_log_scaled.iloc[split_index:]\n",
        "\n",
        "# Plot transformed series\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(ts_log_scaled, label='Log + Scaled Close Price', color='orange')\n",
        "plt.title('Log + Scaled Snapchat Close Price Series')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Transformed Value')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "ts_train_scaled.head(), ts_test_scaled.head()\n"
      ],
      "metadata": {
        "id": "XLyLXrRIK-5g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# ARIMA model (1,1,1) on log+scaled data\n",
        "arima_model = ARIMA(ts_train_scaled, order=(1, 1, 1))\n",
        "arima_result = arima_model.fit()\n",
        "arima_forecast = arima_result.forecast(steps=len(ts_test_scaled))\n",
        "\n",
        "# SARIMA model (1,1,1)(1,1,0,5) as a starting config\n",
        "sarima_model = SARIMAX(ts_train_scaled, order=(1, 1, 1), seasonal_order=(1, 1, 0, 5))\n",
        "sarima_result = sarima_model.fit(disp=False)\n",
        "sarima_forecast = sarima_result.forecast(steps=len(ts_test_scaled))\n",
        "\n",
        "# Evaluate both models\n",
        "def evaluate(y_true, y_pred):\n",
        "    return {\n",
        "        \"MSE\": mean_squared_error(y_true, y_pred),\n",
        "        \"MAE\": mean_absolute_error(y_true, y_pred),\n",
        "        \"R²\": r2_score(y_true, y_pred)\n",
        "    }\n",
        "\n",
        "arima_metrics = evaluate(ts_test_scaled, arima_forecast)\n",
        "sarima_metrics = evaluate(ts_test_scaled, sarima_forecast)\n",
        "\n",
        "# Plot forecasts\n",
        "plt.figure(figsize=(14, 6))\n",
        "plt.plot(ts_train_scaled, label='Train')\n",
        "plt.plot(ts_test_scaled, label='Test', color='black')\n",
        "plt.plot(ts_test_scaled.index, arima_forecast, label='ARIMA Forecast', color='blue')\n",
        "plt.plot(ts_test_scaled.index, sarima_forecast, label='SARIMA Forecast', color='green')\n",
        "plt.title(\"ARIMA vs SARIMA Forecast on Log+Scaled Series\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Transformed Close Price\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "arima_metrics, sarima_metrics\n"
      ],
      "metadata": {
        "id": "3uytb7x5LmMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ARIMA outperformed SARIMA on all metrics (lower error, higher R² — though still negative)\n",
        "\n",
        " SARIMA is overfitting or using ineffective seasonality\n",
        " R² is still negative for both: this means neither model is generalizing well yet (probably due to noise or nonlinearity)\n",
        "\n"
      ],
      "metadata": {
        "id": "dVkasPoxL-Vu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Prophet"
      ],
      "metadata": {
        "id": "OtkKM1IKBw4I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install prophet\n",
        "\n",
        "import pandas as pd\n",
        "from prophet import Prophet\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "snap=pd.read_csv(r'https://github.com/Kanchan786786/Capstone_project-/raw/main/snapchat_stock_data_features.csv')\n",
        "snap.head()\n",
        "\n",
        "snap.info()\n",
        "\n",
        "import pandas as pd\n",
        "from prophet import Prophet\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Prepare the DataFrame for Prophet with log-transformed 'y'\n",
        "df = snap[['Date', 'Close']].copy()\n",
        "df.rename(columns={'Date': 'ds', 'Close': 'y'}, inplace=True)\n",
        "df['ds'] = pd.to_datetime(df['ds'])\n",
        "\n",
        "# Log-transform the target\n",
        "df['y'] = np.log(df['y'])\n",
        "\n",
        "# Step 2: Split into train (70%), validation (10%), test (20%)\n",
        "n = len(df)\n",
        "train_end = int(n * 0.7)\n",
        "val_end = int(n * 0.8)\n",
        "\n",
        "train_df = df.iloc[:train_end]\n",
        "val_df = df.iloc[train_end:val_end]\n",
        "test_df = df.iloc[val_end:]\n",
        "\n",
        "# Step 3: Train the Prophet model\n",
        "model = Prophet(daily_seasonality=True)\n",
        "model.fit(train_df)\n",
        "\n",
        "# Step 4: Create future dataframe and forecast\n",
        "future = model.make_future_dataframe(periods=len(val_df) + len(test_df), freq='D')\n",
        "forecast = model.predict(future)\n",
        "\n",
        "# Step 5: Split forecast for validation and testing\n",
        "forecast_val = forecast.iloc[train_end:val_end][['ds', 'yhat']].set_index('ds')\n",
        "forecast_test = forecast.iloc[val_end:][['ds', 'yhat']].set_index('ds')\n",
        "\n",
        "# Prepare actuals\n",
        "actual_val = val_df.set_index('ds')\n",
        "actual_test = test_df.set_index('ds')\n",
        "\n",
        "# Step 6: Inverse log to get original scale\n",
        "forecast_val['yhat'] = np.exp(forecast_val['yhat'])\n",
        "forecast_test['yhat'] = np.exp(forecast_test['yhat'])\n",
        "actual_val['y'] = np.exp(actual_val['y'])\n",
        "actual_test['y'] = np.exp(actual_test['y'])\n",
        "\n",
        "# Step 7: Evaluate performance\n",
        "val_r2 = r2_score(actual_val['y'], forecast_val['yhat'])\n",
        "val_mse = mean_squared_error(actual_val['y'], forecast_val['yhat'])\n",
        "\n",
        "test_r2 = r2_score(actual_test['y'], forecast_test['yhat'])\n",
        "test_mse = mean_squared_error(actual_test['y'], forecast_test['yhat'])\n",
        "\n",
        "print(f\"Validation R-squared: {val_r2:.4f}\")\n",
        "print(f\"Validation MSE: {val_mse:.4f}\")\n",
        "print(f\"Testing R-squared: {test_r2:.4f}\")\n",
        "print(f\"Testing MSE: {test_mse:.4f}\")\n",
        "\n",
        "# Step 8: Plot forecast (auto scales in log, so reverse manually if needed)\n",
        "model.plot(forecast)\n",
        "plt.title(\"Prophet Forecast of SNAP Stock (Log Transformed + Inverse)\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Close Price\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "from prophet import Prophet\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Step 1: Prepare the DataFrame for Prophet with log-transformed 'y'\n",
        "df = snap[['Date', 'High']].copy()\n",
        "df.rename(columns={'Date': 'ds', 'High': 'y'}, inplace=True)\n",
        "df['ds'] = pd.to_datetime(df['ds'])\n",
        "\n",
        "# Log-transform the target\n",
        "df['y'] = np.log(df['y'])\n",
        "\n",
        "# Step 2: Split into train (70%), validation (10%), test (20%)\n",
        "n = len(df)\n",
        "train_end = int(n * 0.7)\n",
        "val_end = int(n * 0.8)\n",
        "\n",
        "train_df = df.iloc[:train_end]\n",
        "val_df = df.iloc[train_end:val_end]\n",
        "test_df = df.iloc[val_end:]\n",
        "\n",
        "# Step 3: Train the Prophet model\n",
        "model = Prophet(daily_seasonality=True)\n",
        "model.fit(train_df)\n",
        "\n",
        "# Step 4: Create future dataframe and forecast\n",
        "future = model.make_future_dataframe(periods=len(val_df) + len(test_df), freq='D')\n",
        "forecast = model.predict(future)\n",
        "\n",
        "# Step 5: Split forecast for validation and testing\n",
        "forecast_val = forecast.iloc[train_end:val_end][['ds', 'yhat']].set_index('ds')\n",
        "forecast_test = forecast.iloc[val_end:][['ds', 'yhat']].set_index('ds')\n",
        "\n",
        "# Prepare actuals\n",
        "actual_val = val_df.set_index('ds')\n",
        "actual_test = test_df.set_index('ds')\n",
        "\n",
        "# Step 6: Inverse log to get original scale\n",
        "forecast_val['yhat'] = np.exp(forecast_val['yhat'])\n",
        "forecast_test['yhat'] = np.exp(forecast_test['yhat'])\n",
        "actual_val['y'] = np.exp(actual_val['y'])\n",
        "actual_test['y'] = np.exp(actual_test['y'])\n",
        "\n",
        "# Step 7: Evaluate performance\n",
        "val_r2 = r2_score(actual_val['y'], forecast_val['yhat'])\n",
        "val_mse = mean_squared_error(actual_val['y'], forecast_val['yhat'])\n",
        "\n",
        "test_r2 = r2_score(actual_test['y'], forecast_test['yhat'])\n",
        "test_mse = mean_squared_error(actual_test['y'], forecast_test['yhat'])\n",
        "\n",
        "print(f\"Validation R-squared: {val_r2:.4f}\")\n",
        "print(f\"Validation MSE: {val_mse:.4f}\")\n",
        "print(f\"Testing R-squared: {test_r2:.4f}\")\n",
        "print(f\"Testing MSE: {test_mse:.4f}\")\n",
        "\n",
        "# Step 8: Plot forecast (auto scales in log, so reverse manually if needed)\n",
        "model.plot(forecast)\n",
        "plt.title(\"Prophet Forecast of SNAP Stock (Log Transformed + Inverse)\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Price\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Components\n",
        "model.plot_components(forecast)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "0ChFAQAtL6x0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from prophet import Prophet\n",
        "from prophet.diagnostics import cross_validation, performance_metrics\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Load data\n",
        "snap = pd.read_csv('https://github.com/Kanchan786786/Capstone_project-/raw/main/snapchat_stock_data_features.csv')\n",
        "df = snap[['Date', 'Close']].copy()\n",
        "df.rename(columns={'Date': 'ds', 'Close': 'y'}, inplace=True)\n",
        "df['ds'] = pd.to_datetime(df['ds'])\n",
        "\n",
        "# Log transform\n",
        "df['y'] = np.log(df['y'])\n",
        "\n",
        "# Split data\n",
        "n = len(df)\n",
        "train_end = int(n * 0.7)\n",
        "val_end = int(n * 0.8)\n",
        "\n",
        "train_df = df.iloc[:train_end]\n",
        "val_df = df.iloc[train_end:val_end]\n",
        "test_df = df.iloc[val_end:]\n",
        "\n",
        "# Improved Prophet Model\n",
        "model = Prophet(\n",
        "    daily_seasonality=True,\n",
        "    changepoint_prior_scale=0.2  # allow more flexible trend\n",
        ")\n",
        "model.add_seasonality(name='monthly', period=30.5, fourier_order=5)  # custom seasonality\n",
        "model.fit(train_df)\n",
        "\n",
        "# Create future df\n",
        "future = model.make_future_dataframe(periods=len(val_df) + len(test_df), freq='D')\n",
        "forecast = model.predict(future)\n",
        "\n",
        "# Evaluation\n",
        "forecast_val = forecast.iloc[train_end:val_end][['ds', 'yhat']].set_index('ds')\n",
        "forecast_test = forecast.iloc[val_end:][['ds', 'yhat']].set_index('ds')\n",
        "actual_val = val_df.set_index('ds')\n",
        "actual_test = test_df.set_index('ds')\n",
        "\n",
        "forecast_val['yhat'] = np.exp(forecast_val['yhat'])\n",
        "forecast_test['yhat'] = np.exp(forecast_test['yhat'])\n",
        "actual_val['y'] = np.exp(actual_val['y'])\n",
        "actual_test['y'] = np.exp(actual_test['y'])\n",
        "\n",
        "# Scores\n",
        "val_r2 = r2_score(actual_val['y'], forecast_val['yhat'])\n",
        "val_mse = mean_squared_error(actual_val['y'], forecast_val['yhat'])\n",
        "test_r2 = r2_score(actual_test['y'], forecast_test['yhat'])\n",
        "test_mse = mean_squared_error(actual_test['y'], forecast_test['yhat'])\n",
        "\n",
        "print(f\"[IMPROVED] Validation R-squared: {val_r2:.4f}\")\n",
        "print(f\"[IMPROVED] Validation MSE: {val_mse:.4f}\")\n",
        "print(f\"[IMPROVED] Testing R-squared: {test_r2:.4f}\")\n",
        "print(f\"[IMPROVED] Testing MSE: {test_mse:.4f}\")\n",
        "\n",
        "# Component plot\n",
        "model.plot_components(forecast)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Cross-validation\n",
        "df_cv = cross_validation(model, initial='365 days', period='30 days', horizon='90 days')\n",
        "df_perf = performance_metrics(df_cv)\n",
        "print(df_perf.head())\n",
        "\n",
        "# --- END OF IMPROVED VERSION ---\n"
      ],
      "metadata": {
        "id": "eHug1xKvClWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "VZeJlTVIujHc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from prophet import Prophet\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- THIRD PROPHET MODEL (Advanced Strategy with Regressors + Seasonality) ---\n",
        "\n",
        "# Step 1: Load and clean data\n",
        "snap_df = pd.read_csv('https://github.com/Kanchan786786/Capstone_project-/raw/main/snapchat_stock_data_features.csv')\n",
        "df3 = snap_df.copy()\n",
        "df3 = df3.iloc[1:].copy()  # Remove invalid first row\n",
        "df3['Date'] = pd.to_datetime(df3['Date'])\n",
        "\n",
        "# Convert relevant columns to numeric\n",
        "for col in ['Close', 'Open', 'Low', 'High', 'Volume']:\n",
        "    df3[col] = pd.to_numeric(df3[col], errors='coerce')\n",
        "\n",
        "df3.dropna(subset=['Date', 'Close', 'Volume', 'RSI', 'MACD'], inplace=True)\n",
        "df3 = df3.sort_values('Date').reset_index(drop=True)\n",
        "\n",
        "# Step 2: Prepare data for Prophet\n",
        "prophet_df = df3[['Date', 'Close', 'Volume', 'RSI', 'MACD']].copy()\n",
        "prophet_df.rename(columns={'Date': 'ds', 'Close': 'y'}, inplace=True)\n",
        "\n",
        "# Log transform the target\n",
        "prophet_df['y'] = np.log(prophet_df['y'])\n",
        "\n",
        "# Step 3: Split into train, validation, and test\n",
        "n = len(prophet_df)\n",
        "train_end = int(n * 0.7)\n",
        "val_end = int(n * 0.8)\n",
        "\n",
        "train_df = prophet_df.iloc[:train_end]\n",
        "val_df = prophet_df.iloc[train_end:val_end]\n",
        "test_df = prophet_df.iloc[val_end:]\n",
        "\n",
        "# Step 4: Define the Prophet model\n",
        "model = Prophet(\n",
        "    daily_seasonality=False,\n",
        "    yearly_seasonality=True,\n",
        "    weekly_seasonality=True,\n",
        "    changepoint_prior_scale=0.2\n",
        ")\n",
        "model.add_seasonality(name='monthly', period=30.5, fourier_order=5)\n",
        "model.add_regressor('Volume')\n",
        "model.add_regressor('RSI')\n",
        "model.add_regressor('MACD')\n",
        "\n",
        "# Step 5: Fit the model\n",
        "model.fit(train_df)\n",
        "\n",
        "# Step 6: Forecast\n",
        "future = model.make_future_dataframe(periods=len(val_df) + len(test_df), freq='D')\n",
        "future = future.merge(prophet_df[['ds', 'Volume', 'RSI', 'MACD']], on='ds', how='left')\n",
        "\n",
        "# ✅ Fill missing regressor values with forward fill to avoid NaNs\n",
        "future[['Volume', 'RSI', 'MACD']] = future[['Volume', 'RSI', 'MACD']].fillna(method='ffill')\n",
        "\n",
        "# Make predictions\n",
        "forecast = model.predict(future)\n",
        "\n",
        "# Step 7: Evaluation\n",
        "forecast_val = forecast.iloc[train_end:val_end].set_index('ds')\n",
        "forecast_test = forecast.iloc[val_end:].set_index('ds')\n",
        "actual_val = val_df.set_index('ds')\n",
        "actual_test = test_df.set_index('ds')\n",
        "\n",
        "forecast_val['yhat'] = np.exp(forecast_val['yhat'])\n",
        "forecast_test['yhat'] = np.exp(forecast_test['yhat'])\n",
        "actual_val['y'] = np.exp(actual_val['y'])\n",
        "actual_test['y'] = np.exp(actual_test['y'])\n",
        "\n",
        "val_r2 = r2_score(actual_val['y'], forecast_val['yhat'])\n",
        "test_r2 = r2_score(actual_test['y'], forecast_test['yhat'])\n",
        "\n",
        "print(f\"[THIRD MODEL] Validation R²: {val_r2:.4f}\")\n",
        "print(f\"[THIRD MODEL] Testing R²: {test_r2:.4f}\")\n",
        "\n",
        "# Step 8: Plot forecast\n",
        "model.plot(forecast)\n",
        "plt.title(\"Third Prophet Model with Regressors\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lEhGWxr0Xt9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install fbprophet\n",
        "!pip install yfinance"
      ],
      "metadata": {
        "id": "LIzJjT6EynN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt                                                 #Importing matplotlib to plot and analyse data.\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "from pandas import read_csv                                                  #Importing prophet (prediction and forecasting library.)\n",
        "import yfinance as yf"
      ],
      "metadata": {
        "id": "a8sjlUQkurYi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "\n",
        "# Step 1: Download data\n",
        "df_Vi = yf.download('SNAP', start='2019-01-01', end='2024-12-31')\n",
        "\n",
        "# Step 2: Reset index\n",
        "df_Vi.reset_index(inplace=True)\n",
        "\n",
        "# Step 3: Extract clean Series manually (not from DataFrame column directly)\n",
        "date_col = df_Vi['Date'].values.ravel()      # 1D array\n",
        "close_col = df_Vi['Close'].values.ravel()    # ✅ Converts (1509, 1) → (1509,)\n",
        "\n",
        "# Step 4: Plot using flat arrays\n",
        "fig = px.line(x=date_col, y=close_col, title='Snapchat Stock Price Over Time', labels={'x': 'Date', 'y': 'Close'})\n",
        "fig.update_xaxes(rangeslider_visible=True)\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "fr9thDLm191Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_Vi[['ds','y']] = df_Vi[['Date','Close']]                       #preparing expected column names\n",
        "df_Vi"
      ],
      "metadata": {
        "id": "XkQyiSwm3Gc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = df_Vi.sample(frac=0.8, random_state=0)\n",
        "test_data = df_Vi.drop(train_data.index)\n",
        "print(f'training data size : {train_data.shape}')\n",
        "print(f'testing data size : {test_data.shape}')"
      ],
      "metadata": {
        "id": "4AaQhp-A2KXR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "nF0d-uKj3GMu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from prophet import Prophet\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# STEP 2: Split the data\n",
        "n = len(df_Vi)\n",
        "train_end = int(n * 0.7)\n",
        "val_end = int(n * 0.8)\n",
        "\n",
        "train_data = df_Vi.iloc[:train_end]\n",
        "test_data = df_Vi.iloc[val_end:]\n",
        "\n",
        "# STEP 3: Train the model\n",
        "model = Prophet(daily_seasonality=False,weekly_seasonality=False,holidays_prior_scale=5)\n",
        "model.fit(train_data)\n",
        "\n",
        "# STEP 4: Create future dataframe with only test dates\n",
        "future = pd.DataFrame({'ds': test_data['ds']})\n",
        "\n",
        "# STEP 5: Make prediction for test period\n",
        "prediction = model.predict(future)\n",
        "\n",
        "# STEP 6: Extract actual and predicted\n",
        "y_actual = test_data['y']\n",
        "y_predicted = prediction['yhat']\n",
        "\n",
        "# If y was log-transformed, apply inverse\n",
        "y_actual = np.exp(y_actual)\n",
        "y_predicted = np.exp(y_predicted)\n",
        "\n",
        "# STEP 7: Evaluate\n",
        "mae = mean_absolute_error(y_actual, y_predicted)\n",
        "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n"
      ],
      "metadata": {
        "id": "h-gZH3gZ383t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "r2 = r2_score(y_actual, y_predicted)\n",
        "print(f\"📈 R-squared (R²): {r2:.4f}\")\n"
      ],
      "metadata": {
        "id": "VKaYz5zO4hag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(test_data['ds'], y_predicted, 'k')\n",
        "plt.plot(test_data['ds'], y_actual, 'y')\n",
        "plt.xlabel(\"Year\")\n",
        "plt.ylabel(\"Price Action\")\n",
        "plt.title(\"Price Action: Predicted vs Actual\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_ZN1fp645FdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "from prophet import Prophet\n",
        "import matplotlib.pyplot as plt\n",
        "from prophet.diagnostics import cross_validation, performance_metrics\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "# Step 1: Download stock data\n",
        "ticker = \"SNAP\"\n",
        "df = yf.download(ticker, start=\"2024-01-01\", end=\"2025-01-01\", auto_adjust=True)\n",
        "if df.empty:\n",
        "    raise Exception(\"No data found.\")\n",
        "\n",
        "# Step 2: Prepare for Prophet\n",
        "if isinstance(df.columns, pd.MultiIndex):\n",
        "    df = df.xs(ticker, axis=1, level='Ticker')\n",
        "df.reset_index(inplace=True)\n",
        "df_prophet = df[['Date', 'Close']].rename(columns={'Date': 'ds', 'Close': 'y'})\n",
        "df_prophet['ds'] = pd.to_datetime(df_prophet['ds'])\n",
        "df_prophet['y'] = pd.to_numeric(df_prophet['y'], errors='coerce')\n",
        "if df_prophet['y'].isnull().any():\n",
        "    raise Exception(\"Missing values in target (y).\")\n",
        "\n",
        "# Step 3: Build Prophet Model (optimized)\n",
        "model = Prophet(\n",
        "    yearly_seasonality=True,\n",
        "    weekly_seasonality=True,\n",
        "    daily_seasonality=False,\n",
        "    changepoint_prior_scale=0.1,\n",
        "    interval_width=0.90\n",
        ")\n",
        "model.add_seasonality(name='weekly_custom', period=7, fourier_order=3)\n",
        "model.add_seasonality(name='yearly_custom', period=365.25, fourier_order=10)\n",
        "\n",
        "model.fit(df_prophet)\n",
        "\n",
        "# Step 4: In-sample error\n",
        "forecast_train = model.predict(df_prophet)\n",
        "y_true = df_prophet['y'].values\n",
        "y_pred = forecast_train['yhat'].values\n",
        "train_mape = np.mean(np.abs((y_true - y_pred) / y_true))\n",
        "train_rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "train_mae = mean_absolute_error(y_true, y_pred)\n",
        "\n",
        "print(\"🔍 In-Sample Training Metrics:\")\n",
        "print(f\"MAPE: {train_mape:.2%}, RMSE: {train_rmse:.2f}, MAE: {train_mae:.2f}\")\n",
        "\n",
        "# Step 5: Cross-validation\n",
        "df_cv = cross_validation(model, initial='180 days', period='30 days', horizon='90 days')\n",
        "df_metrics = performance_metrics(df_cv)\n",
        "print(\"\\n📊 Cross-Validation Metrics (First 5):\")\n",
        "print(df_metrics[['horizon', 'mape', 'rmse', 'mae']].head())\n",
        "\n",
        "# Optional: Overfitting check\n",
        "cv_mape_mean = df_metrics['mape'].mean()\n",
        "if cv_mape_mean > 1.5 * train_mape:\n",
        "    print(\"⚠️ Model may be overfitting (CV MAPE >> Train MAPE)\")\n",
        "\n",
        "# Step 6: Future forecast\n",
        "future = model.make_future_dataframe(periods=365)\n",
        "forecast = model.predict(future)\n",
        "forecast_2025 = forecast[forecast['ds'].dt.year == 2025]\n",
        "\n",
        "# Step 7: Plot\n",
        "fig = model.plot(forecast_2025)\n",
        "plt.title(\"📈 SNAP Stock Price Forecast for 2025 – Prophet v5\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Price ($)\")\n",
        "plt.show()\n",
        "\n",
        "# Final output\n",
        "print(\"\\n📅 Forecast Summary (Last 5 Days):\")\n",
        "print(forecast_2025[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail())\n"
      ],
      "metadata": {
        "id": "4PQSi7Ik5RRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import r2_score, mean_squared_error"
      ],
      "metadata": {
        "id": "FM9p5qNpjLDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "upf2X4FFin_b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Step 6.5: Evaluate Testing Error on last 90 days of available data\n",
        "# Create test set (last 90 days of historical data)\n",
        "test_start_date = df_prophet['ds'].max() - pd.Timedelta(days=90)\n",
        "df_test = df_prophet[df_prophet['ds'] >= test_start_date]\n",
        "\n",
        "# Get forecasted values for the same dates\n",
        "forecast_test = forecast[forecast['ds'].isin(df_test['ds'])]\n",
        "\n",
        "# Align predictions with actuals\n",
        "y_test_true = df_test.set_index('ds').loc[forecast_test['ds']]['y'].values\n",
        "y_test_pred = forecast_test['yhat'].values\n",
        "\n",
        "# Calculate metrics\n",
        "test_r2 = r2_score(y_test_true, y_test_pred)\n",
        "test_rmse = np.sqrt(mean_squared_error(y_test_true, y_test_pred))\n",
        "test_mae = mean_absolute_error(y_test_true, y_test_pred)\n",
        "test_mape = np.mean(np.abs((y_test_true - y_test_pred) / y_test_true))\n",
        "\n",
        "print(\"\\n🧪 Testing Metrics (Last 90 Days of 2024):\")\n",
        "print(f\"R²: {test_r2:.4f}, RMSE: {test_rmse:.2f}, MAE: {test_mae:.2f}, MAPE: {test_mape:.2%}\")\n"
      ],
      "metadata": {
        "id": "0Y7kr6XhjX2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SVM"
      ],
      "metadata": {
        "id": "GXvdw-toqzN7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install keras-tuner"
      ],
      "metadata": {
        "id": "-1iPdwiFq9LR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from keras_tuner import RandomSearch\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "KjQihToRqy6B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "snap=pd.read_csv(r'https://github.com/Kanchan786786/Capstone_project-/raw/main/snapchat_stock_data_features.csv')\n",
        "snap.head()"
      ],
      "metadata": {
        "id": "IrhjXJZAqy2s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "snap['Date']=pd.to_datetime(snap['Date'])"
      ],
      "metadata": {
        "id": "0KDBhrpeqyz8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "snap.set_index('Date', inplace=True)\n"
      ],
      "metadata": {
        "id": "oGAnDTJgVuO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "snap.info()"
      ],
      "metadata": {
        "id": "eomVi1MCqyve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.svm import SVR  # Import the SVM Regressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "features = ['High', 'Low', 'Open','Adjusted_Close', 'Market_Cap', 'SMA_50', 'EMA_50']\n",
        "target = 'Close'\n",
        "# Split the data into training (80%) and testing (20%) sets\n",
        "train, test = train_test_split(snap, test_size=0.2, random_state=1, shuffle=False)\n",
        "\n",
        "# Initialize the SVM model (SVR for regression) without scaling\n",
        "svm_model = SVR(kernel=\"rbf\", C=1.0, gamma=\"scale\")\n",
        "\n",
        "# Train the SVM model\n",
        "svm_model.fit(train[features], train[target])\n",
        "\n",
        "# Predict on training and testing sets\n",
        "#train_preds = svm_model.predict(train[features])\n",
        "test_preds = svm_model.predict(test[features])\n",
        "\n",
        "# Calculate R-squared and Mean Squared Error (MSE)\n",
        "#train_r2 = r2_score(train[target], train_preds)\n",
        "test_r2 = r2_score(test[target], test_preds)\n",
        "#train_mse = mean_squared_error(train[target], train_preds)\n",
        "test_mse = mean_squared_error(test[target], test_preds)\n",
        "\n",
        "# Print results\n",
        "#print(f\"Training R-squared: {train_r2:.4f}\")\n",
        "print(f\"Testing R-squared: {test_r2:.4f}\")\n",
        "#print(f\"Training Mean Squared Error: {train_mse:.4f}\")\n",
        "print(f\"Testing Mean Squared Error: {test_mse:.4f}\")"
      ],
      "metadata": {
        "id": "Ns7f9wrTqysI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## this model is not performaing well with base model\n",
        "second SVM is with MinMAx scaler\n",
        "\n"
      ],
      "metadata": {
        "id": "JQi5nQkEtVbx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.svm import SVR  # Import the SVM Regressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "features = ['High', 'Low', 'Open','Adjusted_Close', 'Market_Cap', 'SMA_50', 'EMA_50']\n",
        "target = 'Close'\n",
        "# Normalize data using MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "df_scaled = pd.DataFrame(scaler.fit_transform(snap[features + [target]]), columns=features + [target],index=snap.index)\n",
        "# Split the data into training (80%) and testing (20%) sets\n",
        "train, test = train_test_split(df_scaled, test_size=0.2, random_state=1, shuffle=False)\n",
        "\n",
        "# Initialize the SVM model (SVR for regression) without scaling\n",
        "svm_model_2 = SVR(kernel=\"rbf\", C=1.0, gamma=\"scale\")\n",
        "\n",
        "# Train the SVM model\n",
        "svm_model_2.fit(train[features], train[target])\n",
        "\n",
        "# Predict on training and testing sets\n",
        "#train_preds = svm_model.predict(train[features])\n",
        "test_preds = svm_model_2.predict(test[features])\n",
        "\n",
        "# Calculate R-squared and Mean Squared Error (MSE)\n",
        "#train_r2 = r2_score(train[target], train_preds)\n",
        "test_r2 = r2_score(test[target], test_preds)\n",
        "#train_mse = mean_squared_error(train[target], train_preds)\n",
        "test_mse = mean_squared_error(test[target], test_preds)\n",
        "\n",
        "# Print results\n",
        "#print(f\"Training R-squared: {train_r2:.4f}\")\n",
        "print(f\"Testing R-squared: {test_r2:.4f}\")\n",
        "#print(f\"Training Mean Squared Error: {train_mse:.4f}\")\n",
        "print(f\"Testing Mean Squared Error: {test_mse:.4f}\")\n"
      ],
      "metadata": {
        "id": "BOgYsuUlqyn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Comparison of the two SVM we can see the scaled one provide the better results\n",
        "Next we have the next approach where we will take the next approach with taking three parts and"
      ],
      "metadata": {
        "id": "eEoMH9exuo1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tBvBgC9BUCcI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- Load & Prepare Data ---a\n",
        "# Ensure Date column is datetime\n",
        "# Drop the 'Tomorrow' column if it leaks future info\n",
        "features = ['High', 'Low', 'Open', 'Adjusted_Close', 'SMA_50', 'EMA_50']\n",
        "target = 'Close'\n",
        "\n",
        "# --- Time-based Split: 60% Train / 20% Validation / 20% Test ---\n",
        "total_len = len(snap)\n",
        "train_end = int(total_len * 0.6)\n",
        "val_end = int(total_len * 0.8)\n",
        "\n",
        "train_df = snap.iloc[:train_end]\n",
        "val_df = snap.iloc[train_end:val_end]\n",
        "test_df = snap.iloc[val_end:]\n",
        "\n",
        "# Combine Train + Validation for tuning\n",
        "combined_train_val = pd.concat([train_df, val_df])\n",
        "\n",
        "# --- Define Pipeline: Scaling + SVR ---\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', MinMaxScaler()),\n",
        "    ('svr', SVR())\n",
        "])\n",
        "\n",
        "# --- Define Parameter Grid for Hypertuning ---\n",
        "param_grid = {\n",
        "    'svr__C': [0.1, 1, 10],\n",
        "    'svr__gamma': ['scale', 0.01, 0.1],\n",
        "    'svr__epsilon': [0.1, 0.2, 0.3]\n",
        "}\n",
        "\n",
        "# --- Use TimeSeriesSplit for Cross-Validation ---\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "\n",
        "# --- GridSearchCV on Train + Validation ---\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=pipeline,\n",
        "    param_grid=param_grid,\n",
        "    scoring='r2',\n",
        "    cv=tscv,\n",
        "    n_jobs=-1,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "grid_search.fit(combined_train_val[features], combined_train_val[target])\n",
        "\n",
        "# --- Evaluate Best Model ---\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Predict on Test Set\n",
        "test_preds = best_model.predict(test_df[features])\n",
        "\n",
        "# Metrics\n",
        "test_r2 = r2_score(test_df[target], test_preds)\n",
        "test_mse = mean_squared_error(test_df[target], test_preds)\n",
        "\n",
        "print(f\"Best Hyperparameters: {grid_search.best_params_}\")\n",
        "print(f\"Final Test R²: {test_r2:.4f}\")\n",
        "print(f\"Final Test MSE: {test_mse:.4f}\")\n",
        "\n",
        "# --- Plot Actual vs Predicted ---\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(test_df.index, test_df[target], label='Actual', linewidth=2)\n",
        "plt.plot(test_df.index, test_preds, label='Predicted', linestyle='--')\n",
        "plt.title('SVR Predictions vs Actuals')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Close Price')\n",
        "plt.legend()\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "CXdufsvqUATk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "RZBYY2z9UARW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Summary for the company Snapchat\n",
        "\n",
        "\n",
        "| Model                     | Type | Layers/Params                    | Activation Function | Dropout Used | Dropout Rate       | Epochs |  Test Accuracy Range | Tuning Method for improvement           | Key Notes                                       | Outcome                              |\n",
        "|:------------------------:|:----:|:--------------------------------:|:-------------------:|:------------:|:------------------:|:------:|:---------------:|:------------------------:|:-----------------------------------------------:|:------------------------------------:|\n",
        "| LSTM with BayesianOptimization|DL | D50/LSTM_Layer2   | ReLU                | Y            | 0.2 / both layers  | 30   |-0.2286 to   0.68708888 ,0.775157     | Bayesian Optimization     | Moderate improvement, still below Random Search | Slight improvement     and random serach\n",
        "              |\n",
        "| ARIMA                    | TS   | p,d,q (default)                  | N/A                 | N            | N/A                | N/A    |  -0.1374244300   | Random Search           | Non-stationary data; needs differencing         | Try stationarizing data              |\n",
        "| SARIMA                   | TS   | (p,d,q)(P,D,Q,s)                 | N/A                 | N            | N/A                | N/A    | -0.266487648     | Random Search           | Captures seasonality but suffered overfitting   | Overfitting with seasonal terms      |\n",
        "| Prophet                 | TS   | Additive Components              | N/A                 | N            | N/A                | N/A    |  -4.9700   to 0.4567      | basic serach with the  ,cross validation,           | Easy to implement; severe overfitting           | performanced worst at the bse line , with improvised and cross validation step improved .   |\n",
        "| SVM                      | ML   | Kernel=RBF, C=1                  | N/A                 | N            | N/A                | N/A    | range is -7.6 to 0.95           | Random Search (gamma=0.01) | Excellent accuracy, classic model | Best accuracy but still scope to improvised    |\n"
      ],
      "metadata": {
        "id": "ynI4sVeMjpZc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7wnh3r9lkbdU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}